install_github("https://github.com/joachim-gassen/tidycovid19.git")
# If a package is installed, it will be loaded and missing package(s) will be installed
# from CRAN and then loaded.
# The packages we need are:
packages = c("tidyverse", "devtools")
# Load the package or install and load it
package.check <- lapply(
packages,
FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x, dependencies = TRUE)
library(x, character.only = TRUE)
}
}
)
install_github("https://cran.r-project.org/bin/windows/Rtools/")
# If a package is installed, it will be loaded and missing package(s) will be installed
# from CRAN and then loaded.
# The packages we need are:
packages = c("tidyverse", "devtools")
# Load the package or install and load it
package.check <- lapply(
packages,
FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x, dependencies = TRUE)
library(x, character.only = TRUE)
}
}
)
install_github("https://github.com/joachim-gassen/tidycovid19.git")
h
# If a package is installed, it will be loaded and missing package(s) will be installed
# from CRAN and then loaded.
# The packages we need are:
packages = c("tidyverse", "devtools")
# Load the package or install and load it
package.check <- lapply(
packages,
FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x, dependencies = TRUE)
library(x, character.only = TRUE)
}
}
)
install_github("https://github.com/joachim-gassen/tidycovid19.git")
# If a package is installed, it will be loaded and missing package(s) will be installed
# from CRAN and then loaded.
# The packages we need are:
packages = c("tidyverse", "devtools")
# Load the package or install and load it
package.check <- lapply(
packages,
FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x, dependencies = TRUE)
library(x, character.only = TRUE)
}
}
)
install_github("https://github.com/joachim-gassen/tidycovid19.git")
library(tidycovid19)
#Download the data into a data frame called cv.df using the
#download_jhu_csse_covid19_data() function from the {tidycovid19} package.
#
cv.df <- download_jhu_csse_covid19_data(cached = TRUE)
# select only the UK data
cv.uk.df <- subset(cv.df, iso3c=="GBR")
head(cv.uk.df)
tail(cv.uk.df)
# Compute new deaths as the data shows cumulative deaths
cv.uk.df$new.d[2:nrow(cv.uk.df)] <- tail(cv.uk.df$deaths, -1) - head(cv.uk.df$deaths, -1)
cv.uk.df$new.d[1] <- 0     # Add zero for first row
# Compute new infections
cv.uk.df$new.i[2:nrow(cv.uk.df)] <- tail(cv.uk.df$confirmed, -1) - head(cv.uk.df$confirmed, -1)
cv.uk.df$new.i[1] <- 0     # Add zero for first row
# If a package is installed, it will be loaded and missing package(s) will be installed
# from CRAN and then loaded.
# The packages we need are:
packages = c("tidyverse", "devtools")
# Load the package or install and load it
package.check <- lapply(
packages,
FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x, dependencies = TRUE)
library(x, character.only = TRUE)
}
}
)
install_github("https://github.com/joachim-gassen/tidycovid19.git")
library(tidycovid19)
#Download the data into a data frame called cv.df using the
#download_jhu_csse_covid19_data() function from the {tidycovid19} package.
#
cv.df <- download_jhu_csse_covid19_data(cached = TRUE)
# NB a small span value (<1) makes the loess smoother more wiggly!
ggplot(data = cv.uk.df, aes(x = date, y = new.d)) +
geom_line(color = "skyblue", size = 0.6) +
ylim(0,1200) +
stat_smooth(color = "darkorange", fill = "darkorange", method = "loess", span = 0.2) +
ggtitle("Daily additional deaths in the UK due to covid-19") +
xlab("Date") + ylab("Daily new deaths")
ggsave("cv19_UK_deathrate.png")
View(cv.df)
View(cv.uk.df)
# select only the UK data
cv.uk.df <- subset(cv.df, iso3c=="GBR")
head(cv.uk.df)
tail(cv.uk.df)
View(cv.uk.df)
# Compute new deaths as the data shows cumulative deaths
cv.uk.df$new.d[2:nrow(cv.uk.df)] <- tail(cv.uk.df$deaths, -1) - head(cv.uk.df$deaths, -1)
cv.uk.df$new.d[1] <- 0     # Add zero for first row
# Compute new infections
cv.uk.df$new.i[2:nrow(cv.uk.df)] <- tail(cv.uk.df$confirmed, -1) - head(cv.uk.df$confirmed, -1)
cv.uk.df$new.i[1] <- 0     # Add zero for first row
View(cv.uk.df)
# NB a small span value (<1) makes the loess smoother more wiggly!
ggplot(data = cv.uk.df, aes(x = date, y = new.d)) +
geom_line(color = "skyblue", size = 0.6) +
ylim(0,1200) +
stat_smooth(color = "darkorange", fill = "darkorange", method = "loess", span = 0.2) +
ggtitle("Daily additional deaths in the UK due to covid-19") +
xlab("Date") + ylab("Daily new deaths")
ggsave("cv19_UK_deathrate.png")
ggplot(data = cv.uk.df, aes(x = date, y = new.i)) +
geom_line(color = "skyblue", size = 0.6) +
scale_y_continuous(trans = "log10") +
stat_smooth(color = "darkorange", fill = "darkorange", method = "loess") +
ggtitle("Daily new infections in the UK from covid-19") +
xlab("Date") + ylab("Daily new infections")
ggsave("cv19_UK_infectionrate.png")
max(cv.uk.df$new.i, na.rm = FALSE)
cv.fr.df <- subset(cv.df, iso3c=="FRA")
# Compute new deaths as the data shows cumulative deaths
cv.fr.df$new.d[2:nrow(cv.fr.df)] <- tail(cv.fr.df$deaths, -1) - head(cv.fr.df$deaths, -1)
cv.fr.df$new.d[1] <- 0     # Add zero for first row
# Compute new infections
cv.fr.df$new.i[2:nrow(cv.fr.df)] <- tail(cv.fr.df$confirmed, -1) - head(cv.fr.df$confirmed, -1)
cv.fr.df$new.i[1] <- 0     # Add zero for first row
nsamples = 10000
nsamplesmean = 0
nsamplessd = 1
rnormsamples = rnorm(nsamples, nsamplesmean, nsamplessd)
plot(rnormsamples)
hist(rnormsamples)
dnorm(2)
pnorm(nsamplesmean)
qnorm(nsamplesmean)
lab2sol = dbinom(0:5, size = 5, prob = 0.25)
plot(lab2sol)
1-pbinom(0,5,0.25)
##dbinom(x, size, prob, log = FALSE)
##pbinom(q, size, prob, lower.tail = TRUE, log.p = FALSE)
##qbinom(p, size, prob, lower.tail = TRUE, log.p = FALSE)
##rbinom(n, size, prob)
x<-seq(50,150)
y<-dnorm(x,mean = 105, sd=9)
plot(x, dnorm(x,mean = 105, sd=9), type = "l")
plot(x, dnorm(x,mean = 105, sd=9), type = "l")
abline(v=90, lty=2)
abline(v=125, lty=3)
nsamples = 10000
nsamplesmean = 0
nsamplessd = 1
rnormsamples = rnorm(nsamples, nsamplesmean, nsamplessd)
plot(rnormsamples)
hist(rnormsamples)
dnorm(2)
pnorm(nsamplesmean)
qnorm(nsamplesmean)
lab2sol = dbinom(0:5, size = 5, prob = 0.25)
plot(lab2sol)
1-pbinom(0,5,0.25)
##dbinom(x, size, prob, log = FALSE)
##pbinom(q, size, prob, lower.tail = TRUE, log.p = FALSE)
##qbinom(p, size, prob, lower.tail = TRUE, log.p = FALSE)
##rbinom(n, size, prob)
x<-seq(50,150)
y<-dnorm(x,mean = 105, sd=9)
plot(x, dnorm(x,mean = 105, sd=9), type = "l")
plot(x, dnorm(x,mean = 105, sd=9), type = "l")
abline(v=90, lty=2)
abline(v=125, lty=3)
pnorm(125, mean = 105, sd=9)-pnorm(90, mean = 105, sd=9)
qnorm(0.1, mean = 105, sd=9)
library(ggplot2)
ggplot(data=mpg, aes(x=acceleration)) + geom_density() +
theme_classic() + ggtitle("Histogram of Acceletation")
mean.acceleration<-mean(mpg$acceleration)
sd.acceleration<-sd(mpg$acceleration)
library(ggplot2)
ggplot(data=mpg, aes(x=acceleration)) + geom_density() +
theme_classic() + ggtitle("Histogram of Acceletation")
plot(mpg$horsepower,mpg$horsepower,type="l")
library(ggplot2)
ggplot(data=mpg, aes(x=acceleration)) + geom_density() +
theme_classic() + ggtitle("Histogram of Acceletation")
plot(mpg$horsepower,mpg$acceleration,type="l", col = "blue")
mean.acceleration<-mean(mpg$acceleration)
sd.acceleration<-sd(mpg$acceleration)
qnorm(0.975)
ucl<-mean.acceleration-qnorm(0.975)*sd.acceleration/sqrt(398)
lcl<-mean.acceleration+qnorm(0.975)*sd.acceleration/sqrt(398)
ucl
lcl
table(mpg$model.year)
sum(mpg$model.year>79)
mean.acceleration<-mean(mpg$acceleration)
sd.acceleration<-sd(mpg$acceleration)
hist(mpg$acceleration)
nsamples = 10000
nsamplesmean = 0
nsamplessd = 1
rnormsamples = rnorm(nsamples, nsamplesmean, nsamplessd)
plot(rnormsamples)
hist(rnormsamples)
dnorm(2)
pnorm(nsamplesmean)
qnorm(nsamplesmean)
lab2sol = dbinom(0:5, size = 5, prob = 0.25)
plot(lab2sol)
plot(density(rnormsamples))
nsamples = 10000
nsamplesmean = 0
nsamplessd = 1
rnormsamples = rnorm(nsamples, nsamplesmean, nsamplessd)
plot(rnormsamples)
hist(rnormsamples)
dnorm(2)
pnormsamples= pnorm(nsamplesmean)
qnormsamples=qnorm(nsamplesmean)
lab2sol = dbinom(0:5, size = 5, prob = 0.25)
plot(lab2sol)
##this is to get a confidence interval of 95%
plot(density(qnormsamples))
ylab2 = rnorm(1000, mean = 0, sd = 1)
zlab2 = qnorm(0.975)
plot(density(ylab2))
abline(w=zlab2)
text(2,5,0,3, poste("Z=1.95", round(zlab2, digits = 2)), col=rgb(0,0,1,0,9))
ylab2 = rnorm(1000, mean = 0, sd = 1)
zlab2 = qnorm(0.975)
plot(density(ylab2))
abline(w=zlab2)
text(2,5,0,3, paste("Z=1.95", round(zlab2, digits = 2)), col=rgb(0,0,1,0,9))
ylab2 = rnorm(1000, mean = 0, sd = 1)
zlab2 = qnorm(0.975)
plot(density(ylab2))
abline(w=zlab2)
ylab2 = rnorm(1000, mean = 0, sd = 1)
zlab2 = qnorm(0.975)
plot(density(ylab2))
abline(v=zlab2)
text(2,5,0,3, paste("Z=1.95", round(zlab2, digits = 2)), col=rgb(0,0,1,0,9))
ylab2 = rnorm(1000, mean = 0, sd = 1)
zlab2 = qnorm(0.975)
plot(density(ylab2))
abline(v=zlab2)
ylab2 = rnorm(1000, mean = 0, sd = 1)
zlab2 = qnorm(0.975)
zlaba = qnorm(0.025)
plot(density(ylab2))
abline(v=zlab2)
abline(v=zlaba)
ylab2 = rnorm(1000, mean = 0, sd = 1)
zlab2 = qnorm(0.975)
zlaba = qnorm(0.025)
plot(density(ylab2))
abline(v=zlab2,zlaba)
ylab2 = rnorm(1000, mean = 0, sd = 1)
zlab2 = qnorm(0.975)
zlaba = qnorm(0.025)
plot(density(ylab2))
abline(v=zlab2)
abline(v=zlaba)
ucl<-mean.acceleration-qnorm(0.975)*sd.acceleration/sqrt(398)
lcl<-mean.acceleration+qnorm(0.975)*sd.acceleration/sqrt(398)
ucl
lcl
table(mpg$model.year)
sum(mpg$model.year>79)
nsamples = 10000
nsamplesmean = 0
nsamplessd = 1
rnormsamples = rnorm(nsamples, nsamplesmean, nsamplessd)
plot(rnormsamples)
hist(rnormsamples)
dnorm(2)
pnormsamples= pnorm(nsamplesmean)
qnormsamples=qnorm(nsamplesmean)
lab2sol = dbinom(0:5, size = 5, prob = 0.25)
plot(lab2sol)
1-pbinom(0,5,0.25)
##dbinom(x, size, prob, log = FALSE)
##pbinom(q, size, prob, lower.tail = TRUE, log.p = FALSE)
##qbinom(p, size, prob, lower.tail = TRUE, log.p = FALSE)
##rbinom(n, size, prob)
x<-seq(50,150)
y<-dnorm(x,mean = 105, sd=9)
plot(x, dnorm(x,mean = 105, sd=9), type = "l")
plot(x, dnorm(x,mean = 105, sd=9), type = "l")
abline(v=90, lty=2)
abline(v=125, lty=3)
pnorm(125, mean = 105, sd=9)-pnorm(90, mean = 105, sd=9)
qnorm(0.1, mean = 105, sd=9)
mpg<-auto_mpg
summary(mpg)
library(ggplot2)
ggplot(data=mpg, aes(x=acceleration)) + geom_density() +
theme_classic() + ggtitle("Histogram of Acceletation")
plot(mpg$horsepower,mpg$acceleration,type="l", col = "blue")
mean.acceleration<-mean(mpg$acceleration)
sd.acceleration<-sd(mpg$acceleration)
hist(mpg$acceleration)
ylab2 = rnorm(1000, mean = 0, sd = 1)
zlab2 = qnorm(0.975)
zlaba = qnorm(0.025)
plot(density(ylab2))
abline(v=zlab2)
abline(v=zlaba)
ucl<-mean.acceleration-qnorm(0.975)*sd.acceleration/sqrt(398)
lcl<-mean.acceleration+qnorm(0.975)*sd.acceleration/sqrt(398)
table(mpg$model.year)
sum(mpg$model.year>79)
table(mpg$`model year`)
sum(mpg$`model year`>79)
prop.80<-sum(mpg$model.year>79)/nrow(mpg)
prop.80
prop.80<-sum(mpg$`model year`>79)/nrow(mpg)
prop.80
vr<-prop.80*(1-prop.80)/nrow(mpg)
prop.ucl<-prop.80+ qnorm(0.975)*sqrt(vr)
prop.lcl<-prop.80- qnorm(0.975)*sqrt(vr)
test.statistic<-(1.97-2)/(0.1/sqrt(20))
test.statistic<-(1.97-2)/(0.1/sqrt(20))
warnings()
pt(test.statistic, df=19)*2
pt(test.statistic, df=19)*2
qt(0.025, df=19)
test.statistic
test.statistic2<-(1.97-2)/(0.1/sqrt(60))
test.statistic2
pnorm(test.statistic2)
43/50
den<-sqrt((0.75*0.25)/50)
p.test.stat<-(0.86-0.75)/den
p.test.stat
pnorm(p.test.stat)
1-pnorm(p.test.stat)
qnorm(0.95)
1-pnorm(p.test.stat)
head(iris)
setosa.flowers<-subset(iris, iris$Species=="setosa")
versicolor.flowers<-subset(iris, iris$Species=="versicolor")
virginica.flowers<-subset(iris, iris$Species=="virginica")
var(setosa.flowers$Petal.Length)
var(virginica.flowers$Petal.Length)
var.setosa<-var(setosa.flowers$Petal.Length)
var.virginica<-var(virginica.flowers$Petal.Length)
f.st<-var.setosa/var.virginica
f.st
var.test(setosa.flowers$Petal.Length,virginica.flowers$Petal.Length )
t.test(setosa.flowers$Petal.Length,virginica.flowers$Petal.Length)
defectDF <- read.csv(fname, header = TRUE, fileEncoding = 'UTF-8-BOM')
View(defectDF)
# We need to read from the {validate} package in the R library into memory
# If you haven't installed it, then you need to run install.packages("validate")
# but do this only once.
library(validate)
# Store the GitHub address in fname
fname <- "https://raw.githubusercontent.com/mjshepperd/CS5702-Data/master/CS5702_W3_NASAexample.csv"
defectDF <- read.csv(fname, header = TRUE, fileEncoding = 'UTF-8-BOM')
checkResults <- check_that(defectDF, NUMBER_OF_LINES >= LOC_BLANK + LOC_COMMENTS +
LOC_TOTAL)
checkResults
# Take the output from check_that() and turn it into a dataframe f
# or easier manipulation
checkResultsDF <- as.data.frame(checkResults)
problems <- subset(checkResultsDF, value==FALSE)
# Build a rule set (of 2 rules) named rules
rules <- validator(R1 = LOC_CODE_AND_COMMENT >= 0,
R2 = NUMBER_OF_LINES >= (LOC_BLANK + LOC_COMMENTS + LOC_TOTAL))
# Now apply our rule set
checkResults <- confront(defectDF,rules)
checkResultsDF <- as.data.frame(checkResults)
ProbResultsDF <- subset(checkResultsDF, value=="FALSE")
check_that( defectDF$Defective == "Y" | defectDF$Defective == "N")
check_that(defectDF, defectDF$Defective == "Y" | defectDF$Defective == "N")
check_that(defectDF, defectDF$Defective == "Y" | defectDF$Defective == "N")
barplot(check_that(defectDF, defectDF$Defective == "Y" | defectDF$Defective == "N"))
# We need to read from the {validate} package in the R library into memory
# If you haven't installed it, then you need to run install.packages("validate")
# but do this only once.
library(validate)
# Store the GitHub address in fname
fname <- "https://raw.githubusercontent.com/mjshepperd/CS5702-Data/master/CS5702_W3_NASAexample.csv"
defectDF <- read.csv(fname, header = TRUE, fileEncoding = 'UTF-8-BOM')
checkResults <- check_that(defectDF, NUMBER_OF_LINES >= LOC_BLANK + LOC_COMMENTS +
LOC_TOTAL)
checkResults
# Take the output from check_that() and turn it into a dataframe f
# or easier manipulation
checkResultsDF <- as.data.frame(checkResults)
problems <- subset(checkResultsDF, value==FALSE)
# Build a rule set (of 2 rules) named rules
rules <- validator(R1 = LOC_CODE_AND_COMMENT >= 0,
R2 = NUMBER_OF_LINES >= (LOC_BLANK + LOC_COMMENTS + LOC_TOTAL))
# Now apply our rule set
checkResults <- confront(defectDF,rules)
checkResultsDF <- as.data.frame(checkResults)
ProbResultsDF <- subset(checkResultsDF, value=="FALSE")
# Build a rule set (of 2 rules) named rules
rules <- validator(R1 = LOC_CODE_AND_COMMENT >= 0,
R2 = NUMBER_OF_LINES >= (LOC_BLANK + LOC_COMMENTS + LOC_TOTAL))
# Now apply our rule set
checkResults <- confront(defectDF,rules)
checkResultsDF <- as.data.frame(checkResults)
ProbResultsDF <- subset(checkResultsDF, value=="FALSE")
View(ProbResultsDF)
LAACheck <- check_that(LAA,
LAARunsBatted>= 0)
barplot(LAACheck)
BOSCheck <- check_that(BOS,
BOSRunsBatted>= round(0))
barplot(BOSCheck)
#let's look at the problem player
ProblemPlayer <- subset(BOSCheck, value=="FALSE")
LAACheck <- check_that(LAA,
LAARunsBatted>= 0)
barplot(LAACheck)
BOSCheck <- check_that(BOS,
BOSRunsBatted>= round(0))
barplot(BOSCheck)
#let's look at the problem player
ProblemPlayer <- subset(BOSCheck, value=="fail")
LAACheck <- check_that(LAA,
LAARunsBatted>= 0)
barplot(LAACheck)
BOSCheck <- check_that(BOS,
BOSRunsBatted>= round(0))
barplot(BOSCheck)
#let's look at the problem player
ProblemPlayer <- subset(BOSCheck, value==-4)
# Build a rule set (of 2 rules) named rules
rules <- validator(R1 = LOC_CODE_AND_COMMENT >= 0,
R2 = NUMBER_OF_LINES >= (LOC_BLANK + LOC_COMMENTS + LOC_TOTAL))
# Now apply our rule set
checkResults <- confront(defectDF,rules)
checkResultsDF <- as.data.frame(checkResults)
# Take the output from check_that() and turn it into a dataframe f
# or easier manipulation
checkResultsDF <- as.data.frame(checkResults)
problems <- subset(checkResultsDF, value==FALSE)
summary(BOSLAA)
BOSLAACheck <- check_that(BOSLAA,
BOSLAACareer>= 0,
BOSLAARunsBatted>= 0,
BOSLAAGames >= 0,
BOSLAARuns >= 0)
# Produce a bar chart of the quality rule failures
BOSLAACheck
summary(BOSLAA)
BOSLAACheck <- check_that(BOSLAA,
BOSLAACareer>= 0,
BOSLAARunsBatted>= 0,
BOSLAAGames >= 0,
BOSLAARuns >= 0)
# Produce a bar chart of the quality rule failures. I need to turn it into a dataframe
BOSLAACheckDF =  as.data.frame(BOSLAACheck)
LAACheck <- check_that(LAA,
LAARunsBatted>= 0)
barplot(LAACheck)
BOSCheck <- check_that(BOS,
BOSRunsBatted>= round(0))
barplot(BOSCheck)
#let's look at the problem player
ProblemPlayer <- subset(BOSCheck, value==-4)
LAACheck <- check_that(LAA,
LAARunsBatted>= 0)
barplot(LAACheck)
BOSCheck <- check_that(BOS,
BOSRunsBatted>= round(0))
barplot(BOSCheck)
BOSCheckDF =  as.data.frame(BOSCheck)
#let's look at the problem player
ProblemPlayer <- subset(BOSCheck, value==FALSE)
View(BOSCheckDF)
LAACheck <- check_that(LAA,
LAARunsBatted>= 0)
barplot(LAACheck)
BOSCheck <- check_that(BOS,
BOSRunsBatted>= round(0))
barplot(BOSCheck)
BOSCheckDF =  as.data.frame(BOSCheck)
#let's look at the problem player
ProblemPlayer <- subset(BOSCheck, value ==FALSE)
LAACheck <- check_that(LAA,
LAARunsBatted>= 0)
barplot(LAACheck)
BOSCheck <- check_that(BOS,
BOSRunsBatted>= round(0))
barplot(BOSCheck)
BOSCheckDF =  as.data.frame(BOSCheck)
#let's look at the problem player
ProblemPlayer <- subset(BOSCheckDF, value ==FALSE)
ProblemPlayer
View(problems)
View(BOSCheckDF)
View(BOSCheckDF)
