set.seed(2) ## simulate some data...
dat <- gamSim(1,n=400,dist="normal",scale=2)
b <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
summary(b)
plot(b,pages=1,residuals=TRUE)  ## show partial residuals
plot(b,pages=1,seWithMean=TRUE) ## `with intercept' CIs
## run some basic model checks, including checking
## smoothing basis dimensions...
gam.check(b)
## same fit in two parts .....
G <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),fit=FALSE,data=dat)
b <- gam(G=G)
print(b)
## 2 part fit enabling manipulation of smoothing parameters...
G <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),fit=FALSE,data=dat,sp=b$sp)
G$lsp0 <- log(b$sp*10) ## provide log of required sp vec
gam(G=G) ## it's smoother
## change the smoothness selection method to REML
b0 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,method="REML")
## use alternative plotting scheme, and way intervals include
## smoothing parameter uncertainty...
plot(b0,pages=1,scheme=1,unconditional=TRUE)
## Would a smooth interaction of x0 and x1 be better?
## Use tensor product smooth of x0 and x1, basis
## dimension 49 (see ?te for details, also ?t2).
bt <- gam(y~te(x0,x1,k=7)+s(x2)+s(x3),data=dat,
method="REML")
plot(bt,pages=1)
plot(bt,pages=1,scheme=2) ## alternative visualization
AIC(b0,bt) ## interaction worse than additive
## Alternative: test for interaction with a smooth ANOVA
## decomposition (this time between x2 and x1)
bt <- gam(y~s(x0)+s(x1)+s(x2)+s(x3)+ti(x1,x2,k=6),
data=dat,method="REML")
summary(bt)
## If it is believed that x0 and x1 are naturally on
## the same scale, and should be treated isotropically
## then could try...
bs <- gam(y~s(x0,x1,k=40)+s(x2)+s(x3),data=dat,
method="REML")
plot(bs,pages=1)
AIC(b0,bt,bs) ## additive still better.
## Now do automatic terms selection as well
b1 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,
method="REML",select=TRUE)
plot(b1,pages=1)
## set the smoothing parameter for the first term, estimate rest ...
bp <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),sp=c(0.01,-1,-1,-1),data=dat)
plot(bp,pages=1,scheme=1)
## alternatively...
bp <- gam(y~s(x0,sp=.01)+s(x1)+s(x2)+s(x3),data=dat)
# set lower bounds on smoothing parameters ....
bp<-gam(y~s(x0)+s(x1)+s(x2)+s(x3),
min.sp=c(0.001,0.01,0,10),data=dat)
print(b);print(bp)
# same with REML
bp<-gam(y~s(x0)+s(x1)+s(x2)+s(x3),
min.sp=c(0.1,0.1,0,10),data=dat,method="REML")
print(b0);print(bp)
## now a GAM with 3df regression spline term & 2 penalized terms
b0 <- gam(y~s(x0,k=4,fx=TRUE,bs="tp")+s(x1,k=12)+s(x2,k=15),data=dat)
plot(b0,pages=1)
# }
# NOT RUN {
## now simulate poisson data...
set.seed(6)
dat <- gamSim(1,n=2000,dist="poisson",scale=.1)
## use "cr" basis to save time, with 2000 data...
b2<-gam(y~s(x0,bs="cr")+s(x1,bs="cr")+s(x2,bs="cr")+
s(x3,bs="cr"),family=poisson,data=dat,method="REML")
plot(b2,pages=1)
## drop x3, but initialize sp's from previous fit, to
## save more time...
b2a<-gam(y~s(x0,bs="cr")+s(x1,bs="cr")+s(x2,bs="cr"),
family=poisson,data=dat,method="REML",
in.out=list(sp=b2$sp[1:3],scale=1))
par(mfrow=c(2,2))
plot(b2a)
par(mfrow=c(1,1))
## similar example using GACV...
dat <- gamSim(1,n=400,dist="poisson",scale=.25)
b4<-gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=poisson,
data=dat,method="GACV.Cp",scale=-1)
plot(b4,pages=1)
## repeat using REML as in Wood 2011...
b5<-gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=poisson,
data=dat,method="REML")
plot(b5,pages=1)
## a binary example (see ?gam.models for large dataset version)...
dat <- gamSim(1,n=400,dist="binary",scale=.33)
lr.fit <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=binomial,
data=dat,method="REML")
## plot model components with truth overlaid in red
op <- par(mfrow=c(2,2))
fn <- c("f0","f1","f2","f3");xn <- c("x0","x1","x2","x3")
for (k in 1:4) {
plot(lr.fit,residuals=TRUE,select=k)
ff <- dat[[fn[k]]];xx <- dat[[xn[k]]]
ind <- sort.int(xx,index.return=TRUE)$ix
lines(xx[ind],(ff-mean(ff))[ind]*.33,col=2)
}
par(op)
anova(lr.fit)
lr.fit1 <- gam(y~s(x0)+s(x1)+s(x2),family=binomial,
data=dat,method="REML")
lr.fit2 <- gam(y~s(x1)+s(x2),family=binomial,
data=dat,method="REML")
AIC(lr.fit,lr.fit1,lr.fit2)
## For a Gamma example, see ?summary.gam...
## For inverse Gaussian, see ?rig
## now 2D smoothing...
eg <- gamSim(2,n=500,scale=.1)
attach(eg)
op <- par(mfrow=c(2,2),mar=c(4,4,1,1))
contour(truth$x,truth$z,truth$f) ## contour truth
b4 <- gam(y~s(x,z),data=data) ## fit model
fit1 <- matrix(predict.gam(b4,pr,se=FALSE),40,40)
contour(truth$x,truth$z,fit1)   ## contour fit
persp(truth$x,truth$z,truth$f)    ## persp truth
vis.gam(b4)                     ## persp fit
detach(eg)
par(op)
ozone.tree<-tree(ozone.pollution$ozone~., data = ozone.pollution)
plot(ozone.tree)
text(ozone.tree)
m1.lm<-lm(ozone~temp*wind*rad+I(temp^2)+ I(wind^2)+ I(rad^2), data=ozone.pollution )
summary(m1.lm)
m2.lm<-lm(ozone~temp*wind*rad+I(temp^2)+ I(wind^2)+ I(rad^2)-temp:wind:rad, data=ozone.pollution)
summary(m2.lm)
m3.lm<-update(m2.lm,~.-wind:rad)
summary(m3.lm)
m4.lm<-update(m3.lm, ~.-temp:wind)
summary(m4.lm)
m5.lm<-update(m4.lm, ~.-I(rad^2))
summary(m5.lm)
m6.lm<-update(m5.lm, ~.-temp:rad)
summary(m6.lm)
plot(m6.lm)
#log transform
m7.lm<-lm(log(ozone)~temp*wind*rad+I(temp^2)+ I(wind^2)+ I(rad^2),data=ozone.pollution)
summary(m7.lm)
m8.lm<-step(m7.lm)
summary(m8.lm)
plot(m8.lm)
m8.lm<-step(m7.lm)
summary(m8.lm)
print(m8.lm)
m8.lm<-step(m7.lm)
summary(m8.lm)
library(ggplot2)
diet.df<-read.csv("Diet_r.csv")
summary(diet.df)
diet.df$gender<-as.factor(diet.df$gender)
table(diet.df$Diet)
diet.df$Diet<-as.factor(diet.df$Diet)
diet.df$Diet<-as.factor(diet.df$Diet)
ggplot(diet.df, aes(x=Age)) + geom_histogram(bins=10) + theme_bw()
ggplot(diet.df, aes(x=pre.weight)) + geom_histogram(bins=10) + theme_bw()
ggplot(diet.df, aes(x=weight6weeks)) + geom_histogram(bins=10) + theme_bw()
ggplot(diet.df, aes(x=Diet, y=pre.weight, fill=gender)) +geom_boxplot() + theme_bw() +
ggtitle("Distribution of Starting Weight by Diet and Gender")
t.test(diet.df$pre.weight, diet.df$weight6weeks)
diet.df$weight.lost<-diet.df$pre.weight-diet.df$weight6weeks
diet.df$weight.lost<-diet.df$pre.weight-diet.df$weight6weeks
summary(diet.df$weight.lost)
hist(diet.df$weight.lost)
summary(aov(diet.df$weight.lost~diet.df$Diet))
summary.lm(aov(diet.df$weight.lost~diet.df$Diet))
plot(aov(diet.df$weight.lost~diet.df$Diet))
aggregate(diet.df$weight.lost~diet.df$Diet, FUN="mean")
ggplot(diet.df, aes(x=Diet, y=weight.lost)) + geom_boxplot() + theme_classic()
model2<-lm(diet.df$weight.lost~diet.df$Diet+diet.df$Age)
summary(model2)
plot(model2)
model3<-lm(diet.df$weight.lost~diet.df$Diet+ diet.df$gender + diet.df$pre.weight)
summary(model3)
diet.df$diet.ind<-ifelse(diet.df$Diet=="3", "3", "1 or 2")
table(diet.df$diet.ind)
model3<-aov(diet.df$weight.lost~diet.df$diet.ind)
summary.lm(model3)
diet2.df<-subset(diet.df, !is.na(diet.df$gender))
model.gender.aov<-aov(diet2.df$weight.lost~diet2.df$gender)
summary.lm(model.gender.aov)
# This is an R code chunk
print("Goodbye cruel world")
# This is an R code chunk
print("Goodbye cruel world")
z <- 3
z <- z * 17
print(z)
# Fetch the raw data
# Read the csv file into a new data frame called surveyDF
surveyFileName <- "https://raw.githubusercontent.com/mjshepperd/CS5702-Data/master/CS5702JoiningSurvey_2020.csv"
surveyDF <- read.csv(surveyFileName, header = TRUE, stringsAsFactors = FALSE)
# Clean up surveyDF column names
names(surveyDF)  # The names() function returns all the dataframe column names
# Choose shorter, clearer names
names(surveyDF)[2] <- "MSc"
names(surveyDF)[3] <- "StudyMode"
names(surveyDF)[4] <- "Background"
names(surveyDF)[5] <- "Motivation"
names(surveyDF)[6] <- "StatsKnowl"
names(surveyDF)[7] <- "ProgKnowl"
names(surveyDF)[8] <- "ProgLangs"
names(surveyDF)[9] <- "Excite"
names(surveyDF)[10] <- "Concern"
# Fetch the raw data
# Read the csv file into a new data frame called surveyDF
surveyFileName <- "https://raw.githubusercontent.com/mjshepperd/CS5702-Data/master/CS5702JoiningSurvey_2020.csv"
surveyDF <- read.csv(surveyFileName, header = TRUE, stringsAsFactors = FALSE)
print(surveyDF)
z <- 3
z <- z * 17
print(z)
# A *very* simple way to check for missing observations in our data frame
# using the is.na() function
is.na(surveyDF)
# Examine the data frame structure
str(surveyDF)
# Shorten the MSc titles (for convenience)
surveyDF[surveyDF=="MSc Artificial Intelligence (AI)"] <- "AI"
surveyDF[surveyDF=="MSc Data Science Analytics (DSA)"] <- "DSA"
# Using the summary() function on the numeric variables
summary(surveyDF$StatsKnowl)
summary(surveyDF$ProgKnowl)
# And easier to visualise
# I superimpose the a probability density plot over the histogram
hist(surveyDF$StatsKnowl,
main="Histogram of Perceived Statistical Knowledge",
xlab="Score (1 = low, 10 = high)",
col="darkgray",
xlim=c(1,10),
breaks=1:10,
prob = TRUE
)
library("ggplot2", lib.loc="~/R/win-library/4.0")
# Using the summary() function on the numeric variables
summary(surveyDF$StatsKnowl)
summary(surveyDF$ProgKnowl)
# And easier to visualise
# I superimpose the a probability density plot over the histogram
hist(surveyDF$StatsKnowl,
main="Histogram of Perceived Statistical Knowledge",
xlab="Score (1 = low, 10 = high)",
col="darkgray",
xlim=c(1,10),
breaks=1:10,
prob = TRUE
)
# Shorten the MSc titles (for convenience)
surveyDF[surveyDF=="MSc Artificial Intelligence (AI)"] <- "AI"
surveyDF[surveyDF=="MSc Data Science Analytics (DSA)"] <- "DSA"
# Using the summary() function on the numeric variables
summary(surveyDF$StatsKnowl)
summary(surveyDF$ProgKnowl)
# And easier to visualise
# I superimpose the a probability density plot over the histogram
hist(surveyDF$StatsKnowl,
main="Histogram of Perceived Statistical Knowledge",
xlab="Score (1 = low, 10 = high)",
col="darkgray",
xlim=c(1,10),
breaks=1:10,
prob = TRUE
)
# Use the table() function to produce tables of frequency counts
table(surveyDF$MSc)
table(surveyDF$StudyMode)
table(surveyDF$Background)
# Fetch the raw data
# Read the csv file into a new data frame called surveyDF
surveyFileName <- "https://raw.githubusercontent.com/mjshepperd/CS5702-Data/master/CS5702JoiningSurvey_2020.csv"
surveyDF <- read.csv(surveyFileName, header = TRUE, stringsAsFactors = FALSE)
print(surveyDF)
# Fetch the raw data
# Read the csv file into a new data frame called surveyDF
surveyFileName <- "https://raw.githubusercontent.com/mjshepperd/CS5702-Data/master/CS5702JoiningSurvey_2020.csv"
surveyDF <- read.csv(surveyFileName, header = TRUE, stringsAsFactors = FALSE)
print(surveyDF)
view(surveyDF)
View(surveyDF)
# Fetch the raw data
# Read the csv file into a new data frame called surveyDF
surveyFileName <- "https://raw.githubusercontent.com/mjshepperd/CS5702-Data/master/CS5702JoiningSurvey_2020.csv"
surveyDF <- read.csv(surveyFileName, header = TRUE, stringsAsFactors = FALSE)
print(surveyDF)
surveyDF[3,2]
print(survey$Msc)
print(surveyDF$Which.MSc.will.you.be.studying.)
# Clean up surveyDF column names
names(surveyDF)  # The names() function returns all the dataframe column names
# Choose shorter, clearer names
names(surveyDF)[2] <- "MSc"
names(surveyDF)[3] <- "StudyMode"
names(surveyDF)[4] <- "Background"
names(surveyDF)[5] <- "Motivation"
names(surveyDF)[6] <- "StatsKnowl"
names(surveyDF)[7] <- "ProgKnowl"
names(surveyDF)[8] <- "ProgLangs"
names(surveyDF)[9] <- "Excite"
names(surveyDF)[10] <- "Concern"
print(surveyDF$[1])
print(surveyDF$MSc)
# Using the summary() function on the numeric variables
summary(surveyDF$StatsKnowl)
summary(surveyDF$ProgKnowl)
# And easier to visualise
# I superimpose the a probability density plot over the histogram
hist(surveyDF$StatsKnowl,
main="Histogram of Perceived Statistical Knowledge",
xlab="Score (1 = low, 10 = high)",
col="darkgray",
xlim=c(1,10),
breaks=1:10,
prob = TRUE
)
lines(density(surveyDF$StatsKnowl), col = "red")
hist(surveyDF$ProgKnowl,
main="Histogram of Perceived Programming Knowledge",
xlab="Score (1 = low, 10 = high)",
col="darkgray",
xlim=c(1,10),
breaks=1:10,
prob = TRUE
)
lines(density(surveyDF$ProgKnowl), col = "red")
# Alternatively we can use boxplots
boxplot(surveyDF$StatsKnowl, surveyDF$ProgKnowl)
# Use the table() function to produce tables of frequency counts
table(surveyDF$MSc)
table(surveyDF$StudyMode)
table(surveyDF$Background)
# Use the table() function to produce tables of frequency counts
table(surveyDF$MSc)
table(surveyDF$StudyMode)
table(surveyDF$Background)
# This is an example of a 2-d table of frequencies.
table(surveyDF$Background, surveyDF$MSc)
# and if we want marginal totals wrap table() with the addmargins() function
addmargins(table(surveyDF$Background, surveyDF$MSc))
# and if you want proportions then wrap with prop.table()
prop.table(table(surveyDF$Background, surveyDF$MSc))
# Convert MSc into a factor, i.e., use as a categorical variable
surveyDF$MSc <- as.factor(surveyDF$MSc)
# Compare the different distributions of values using a boxplot for each value of the factor MSc.
# Since there are 2 courses (AI and DSA) this produces 2 boxplots of StatsKnowl and we can see
# if there are any differences
boxplot(surveyDF$StatsKnowl ~ surveyDF$MSc,
notch = TRUE,            # Shows the 95% confidence intervals
horizontal = TRUE,
xlab = "Statistical understanding",
ylab = "")
# Side by side boxplots of ProgKnowl separated by the factor MSc.
boxplot(surveyDF$ProgKnowl ~ surveyDF$MSc,
notch = TRUE,           # Shows the 95% confidence intervals
horizontal = TRUE,
xlab = "Programming understanding",
ylab = "")
# To extract all the words into a single character string you need the
# paste() function with the collapse option.
words <- paste(surveyDF$Excite, collapse = " ")
# Display the words
words
# Save the words in a text file as input to a word cloud generator
# You can change the file name
# NB This will overwrite the previous contents (if any)
fileName <- file("MyWords.txt")
writeLines(words, fileName)
close(fileName)
# Compute sales income for Martin's book
price <- 10                                     # assume price is 10 UKP per book copy
numberStudents <- 100                           # number of students in the class
recommend <- 0.5                                # proportion of students who recommend the book
sales <- numberStudents * (1 + recommend)
income <- sales * price
outputMsg <- paste("Martin will earn:", income) # format a readable string with the paste function
print(outputMsg, quote = FALSE)                 # output the concatenated string without quotes
# Your extended code needs to go here
# If a package is installed, it will be loaded and missing package(s) will be installed
# from CRAN and then loaded.
# The packages we need are:
packages = c("tidyverse", "devtools")
# Load the package or install and load it
package.check <- lapply(
packages,
FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x, dependencies = TRUE)
library(x, character.only = TRUE)
}
}
)
install_github("joachim-gassen/tidycovid19")
#Download the data into a data frame called cv.df using the
#download_jhu_csse_covid19_data() function from the {tidycovid19} package.
#
cv.df <- download_jhu_csse_covid19_data(cached = TRUE)
install.packages("tidycovid")
# If a package is installed, it will be loaded and missing package(s) will be installed
# from CRAN and then loaded.
# The packages we need are:
packages = c("tidyverse", "devtools")
# Load the package or install and load it
package.check <- lapply(
packages,
FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x, dependencies = TRUE)
library(x, character.only = TRUE)
}
}
)
install_github("tidycovid19")
# If a package is installed, it will be loaded and missing package(s) will be installed
# from CRAN and then loaded.
# The packages we need are:
packages = c("tidyverse", "devtools")
# Load the package or install and load it
package.check <- lapply(
packages,
FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x, dependencies = TRUE)
library(x, character.only = TRUE)
}
}
)
install_github("https://github.com/joachim-gassen/tidycovid19.git")
library(tidyverse)
library(tidycovid19)
library(zoo)
library(tidycovid19)
#Download the data into a data frame called cv.df using the
#download_jhu_csse_covid19_data() function from the {tidycovid19} package.
#
cv.df <- download_jhu_csse_covid19_data(cached = TRUE)
# select only the UK data
cv.uk.df <- subset(cv.df, iso3c=="GBR")
head(cv.uk.df)
tail(cv.uk.df)
# Compute new deaths as the data shows cumulative deaths
cv.uk.df$new.d[2:nrow(cv.uk.df)] <- tail(cv.uk.df$deaths, -1) - head(cv.uk.df$deaths, -1)
cv.uk.df$new.d[1] <- 0     # Add zero for first row
# Compute new infections
cv.uk.df$new.i[2:nrow(cv.uk.df)] <- tail(cv.uk.df$confirmed, -1) - head(cv.uk.df$confirmed, -1)
cv.uk.df$new.i[1] <- 0     # Add zero for first row
# NB a small span value (<1) makes the loess smoother more wiggly!
ggplot(data = cv.uk.df, aes(x = date, y = new.d)) +
geom_line(color = "skyblue", size = 0.6) +
ylim(0,1200) +
stat_smooth(color = "darkorange", fill = "darkorange", method = "loess", span = 0.2) +
ggtitle("Daily additional deaths in the UK due to covid-19") +
xlab("Date") + ylab("Daily new deaths")
ggsave("cv19_UK_deathrate.png")
ggplot(data = cv.uk.df, aes(x = date, y = new.i)) +
geom_line(color = "skyblue", size = 0.6) +
scale_y_continuous(trans = "log10") +
stat_smooth(color = "darkorange", fill = "darkorange", method = "loess") +
ggtitle("Daily new infections in the UK from covid-19") +
xlab("Date") + ylab("Daily new infections")
ggsave("cv19_UK_infectionrate.png")
is.integer(cv.uk.df$new.i)
cv.uk.df$new.i <- as.integer(cv.uk.df$new.i)
cv.uk.df$new.i <- as.integer(cv.uk.df$new.i)
cv.uk.df$recovered[236] <- 0
# Compute new deaths as the data shows cumulative deaths
cv.uk.df$new.d[2:nrow(cv.uk.df)] <- tail(cv.uk.df$deaths, -1) - head(cv.uk.df$deaths, -1)
cv.uk.df$new.d[1] <- 0     # Add zero for first row
# Compute new infections
cv.uk.df$new.i[2:nrow(cv.uk.df)] <- tail(cv.uk.df$confirmed, -1) - head(cv.uk.df$confirmed, -1)
cv.uk.df$new.i[1] <- 0     # Add zero for first row
# NB a small span value (<1) makes the loess smoother more wiggly!
ggplot(data = cv.uk.df, aes(x = date, y = new.d)) +
geom_line(color = "skyblue", size = 0.6) +
ylim(0,1200) +
stat_smooth(color = "darkorange", fill = "darkorange", method = "loess", span = 0.2) +
ggtitle("Daily additional deaths in the UK due to covid-19") +
xlab("Date") + ylab("Daily new deaths")
ggsave("cv19_UK_deathrate.png")
ggplot(data = cv.uk.df, aes(x = date, y = new.i)) +
geom_line(color = "skyblue", size = 0.6) +
scale_y_continuous(trans = "log10") +
stat_smooth(color = "darkorange", fill = "darkorange", method = "loess") +
ggtitle("Daily new infections in the UK from covid-19") +
xlab("Date") + ylab("Daily new infections")
ggsave("cv19_UK_infectionrate.png")
# We need to read from the {validate} package in the R library into memory
# If you haven't installed it, then you need to run install.packages("validate")
# but do this only once.
library(validate)
install.packages("validate")
# We need to read from the {validate} package in the R library into memory
# If you haven't installed it, then you need to run install.packages("validate")
# but do this only once.
library(validate)
# Store the GitHub address in fname
fname <- "https://raw.githubusercontent.com/mjshepperd/CS5702-Data/master/CS5702_W3_NASAexample.csv"
defectDF <- read.csv(fname, header = TRUE, fileEncoding = 'UTF-8-BOM')
checkResults <- check_that(defectDF, NUMBER_OF_LINES >= LOC_BLANK + LOC_COMMENTS +
LOC_TOTAL)
checkResults
# Take the output from check_that() and turn it into a dataframe f
# or easier manipulation
checkResultsDF <- as.data.frame(checkResults)
problems <- subset(checkResultsDF, value==FALSE)
# Build a rule set (of 2 rules) named rules
rules <- validator(R1 = LOC_CODE_AND_COMMENT >= 0,
R2 = NUMBER_OF_LINES >= (LOC_BLANK + LOC_COMMENTS + LOC_TOTAL))
# Now apply our rule set
checkResults <- confront(defectDF,rules)
checkResultsDF <- as.data.frame(checkResults)
ProbResultsDF <- subset(checkResultsDF, value=="FALSE")
