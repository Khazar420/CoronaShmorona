Cleaning data:
is.na(filename)
finds missing values
str(surveyDF)
structure
mean, median, quartile
(minimum, lower-hinge, median, upper-hinge, maximum) `fivenum()

Metadata
assess the quality
full impletmentation?


```{r}

summary(CS5801.data)
summary(BOSLAA)
summary(BOS)
summary(LAA)

```


----


Hypothesis testing?
Compute a 95% confidence interval for the mean for one of the variable that appears to be normally
6 ways to see  normal distribution:
Histogram - my_data.hist()
Box Plot - my_data.plot(kind = 'box')
QQ Plot - pylab.show()
Kolmogorov Smirnov test - from scipy.stats import kstest, norm
my_data = norm.rvs(size=1000)
ks_statistic, p_value = kstest(my_data, 'norm')
print(ks_statistic, p_value)
Lilliefors test - from scipy.stats import norm
from statsmodels.stats.diagnostic import lilliefors
my_data = norm.rvs(size=500)
lilliefors(my_data)
Shapiro Wilk test - from scipy.stats import norm
from statsmodels.stats import shapiro
my_data = norm.rvs(size=500)
shapiro(my_data)

distributed 
binomial distribution
Poisson distribution
normal distribution
var.test is very important - allows to comparare variances
t.test compares means - welch
ANOVA - Analysis of Variance 
for example: 
cars.lm<-lm(cars$dist~cars$speed, data=cars)
summary(cars.lm)
plot(cars.lm)
also this?cars.lm2<-lm(cars$dist~cars$speed+ I(cars$speed^2))
with more variables:
m1.lm<-lm(ozone~temp*wind*rad+I(temp^2)+ I(wind^2)+ I(rad^2), data=ozone.pollution )
summary(m1.lm)
If you do ANOVA with lots of variables, just get rid of the onces with low P values
ANCOVA - ANOVA with covariance
ancova.iq<-lm(iq$iq~iq$group+iq$age)
summary(ancova.iq)
plot(ancova.iq)

	F test to compare two variances
 Welch t-test
 lcl and ucl in the boxplots
 ggplot(data = iq, aes(x=group, y=age)) +geom_boxplot() + theme_classic()
 ANOVA again summary(aov(iq$iq~iq$group))
 Higher F-statistic is better
 aggregate function - caltulating a specific propterty 
 - aggregate(iq~group, data=iq, FUN="mean")
 Connect several categories if similar enough, or indeed separate if too distinct
 stem and leaf plot
box plot
scatter diagram
histogram
dotplot
probablility plot
table
Ehrenberg's two-variable-digits rule
 transpose
reorder rows or colluns
ugly E numbers should be multiplied or divided by the appropriate order of magnitude in order to maek them readable

pairs(ozone.pollution, panel = panel.smooth)
plot(ozone.pollution)
Pairs apperas the same as plot, except this one gives a nice red trend line
GAM - generalized additive model - loads of rally cool ways to show data
ozone.gam<-gam(ozone.pollution$ozone~s(ozone.pollution$rad)+s(ozone.pollution$temp)+s(ozone.pollution$wind))
summary(ozone.gam)
plot(ozone.gam)
tree - this is very good when doing dependent variables
m8.lm<-step(m7.lm)
summary(m8.lm)
Step creates the minimal adaptive model
AIC (lowerbetter) is like the opposite of R^2 (higherbetter)
AICpenalises for having too many paremetres. It lists models and gives the best at the bottom

```{r}

dim(CS5801.data)

glimpse(BOSLAA)


BOSLAA = subset(CS5801.data, teamID.x == "BOS" | teamID.x=="LAA")
BOS = subset(CS5801.data, teamID.x == "BOS")
LAA = subset(CS5801.data, teamID.x == "LAA")


```

Things to have
ggplot
dplyr
dbplyr
foreign
ggpubr
xtable
mgcv
tree


The first thing that I will do is look at the data as a whole and try to see whenther there is anything that stands out such as unusual figures or trends between all of the teams.

Then I will separate out the data, and take out the two teams that I will example, and see whether there is anything between them.

The third step will be to put all of the team data together, and see whether there is anything that is clear when adding them together.

After these three steps are done, I will try to find and prove any correlants.
The one main issue for the author personally is the numbe of variables. With 15 variables, there needs to be a way to plot everything in a near and tidy way without using too many graphs.
This allows for better legibility, and also makes everything easier to manage without obfuscating anything.

At each step, five things will be looked at and taken into consideration
Classification
Anomaly detection
Quantifying - regression
Cluttering - organising
What next?

This will be a regular cyclical process so as to ensure that by the end of things the right   question is asked appropriately.


```{r}
#this needs to be adapted to the data
# Using the summary() function on the numeric variables
summary(surveyDF$StatsKnowl)
summary(surveyDF$ProgKnowl)

# And easier to visualise
# I superimpose the a probability density plot over the histogram

hist(surveyDF$StatsKnowl, 
     main="Histogram of Perceived Statistical Knowledge", 
     xlab="Score (1 = low, 10 = high)", 
     col="darkgray",
     xlim=c(1,10),
     breaks=1:10,
     prob = TRUE
     )
lines(density(surveyDF$StatsKnowl), col = "red")  

hist(surveyDF$ProgKnowl, 
     main="Histogram of Perceived Programming Knowledge", 
     xlab="Score (1 = low, 10 = high)", 
     col="darkgray",
     xlim=c(1,10),
     breaks=1:10,
     prob = TRUE
     )
lines(density(surveyDF$ProgKnowl), col = "red")  
# Alternatively we can use boxplots
boxplot(surveyDF$StatsKnowl, surveyDF$ProgKnowl)

# Use the table() function to produce tables of frequency counts
table(surveyDF$MSc)
table(surveyDF$StudyMode)
table(surveyDF$Background)

```

**Exercise**: Is there any relationship between `Background` [UG course] and choice of MSc?
relatiohsips between stuff

```{r}
# This is an example of a 2-d table of frequencies.
table(surveyDF$Background, surveyDF$MSc)

# and if we want marginal totals wrap table() with the addmargins() function
addmargins(table(surveyDF$Background, surveyDF$MSc)) 

# and if you want proportions then wrap with prop.table()
prop.table(table(surveyDF$Background, surveyDF$MSc))



'''

```

##Data quality

```{r}
defectDF <- read.csv(fname, header = TRUE, fileEncoding = 'UTF-8-BOM')
```
    

## Check Data Quality

```{r}
## Data quality checking

val.check1 <- check_that(dataQualEg1.DF, age > 0, age > 115, married == "Y" | married == "N")

# Produce a bar chart of the quality rule failures
barplot(val.check1)
```
ANOTHER VERSION HERE:   

We can look for referential integrity problems, i.e., validity is determined in the context of more than one variable.

```{r}
checkResults <- check_that(defectDF, NUMBER_OF_LINES >= LOC_BLANK + LOC_COMMENTS + 
    LOC_TOTAL)
checkResults

```
after that, I can take that, make a dataframe, an then create a subset with the problems to look at them - see week 3 5702
```{r}
# Take the output from check_that() and turn it into a dataframe f
# or easier manipulation
checkResultsDF <- as.data.frame(checkResults)
problems <- subset(checkResultsDF, value==FALSE)
```
